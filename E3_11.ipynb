{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "15e78de6",
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# E3: Multimodal Representations\n"
      ],
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        },
        "id": "15e78de6"
      }
    },
    {
      "id": "kUZsLi_me0-y",
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "kUZsLi_me0-y"
      }
    },
    {
      "id": "b2506580",
      "cell_type": "markdown",
      "source": [
        "### 0.a Dataset creation"
      ],
      "metadata": {
        "id": "b2506580"
      }
    },
    {
      "id": "380d3fbe-0848-4ddf-9c54-872c98ec228d",
      "cell_type": "markdown",
      "source": [
        "Download the necessary data"
      ],
      "metadata": {
        "id": "380d3fbe-0848-4ddf-9c54-872c98ec228d"
      }
    },
    {
      "id": "31e47f66-a3b5-44c6-a9ab-c60b2adfbe5c",
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mlfoundations/imagenet-captions/main/imagenet_captions.zip; unzip imagenet_captions.zip; wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz; tar -xzf imagenette2.tgz"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:33:45.837539Z",
          "iopub.execute_input": "2025-07-03T12:33:45.837812Z",
          "iopub.status.idle": "2025-07-03T12:38:06.554859Z",
          "shell.execute_reply.started": "2025-07-03T12:33:45.837790Z",
          "shell.execute_reply": "2025-07-03T12:38:06.554040Z"
        },
        "id": "31e47f66-a3b5-44c6-a9ab-c60b2adfbe5c",
        "outputId": "9bee578d-a11e-467e-e213-7b19490bf6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--2025-07-03 12:33:45--  https://raw.githubusercontent.com/mlfoundations/imagenet-captions/main/imagenet_captions.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 34493455 (33M) [application/zip]\nSaving to: ‘imagenet_captions.zip’\n\nimagenet_captions.z 100%[===================>]  32.90M   199MB/s    in 0.2s    \n\n2025-07-03 12:33:46 (199 MB/s) - ‘imagenet_captions.zip’ saved [34493455/34493455]\n\nArchive:  imagenet_captions.zip\n  inflating: imagenet_captions.json  \n--2025-07-03 12:33:47--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.198.8, 16.182.74.56, 54.231.130.152, ...\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.198.8|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1557161267 (1.5G) [application/x-tar]\nSaving to: ‘imagenette2.tgz’\n\nimagenette2.tgz     100%[===================>]   1.45G  8.29MB/s    in 4m 8s   \n\n2025-07-03 12:37:55 (5.99 MB/s) - ‘imagenette2.tgz’ saved [1557161267/1557161267]\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "0140b4ce-feba-43ec-9a97-8c0ef53bf799",
      "cell_type": "markdown",
      "source": [
        "Check the data loads correctly"
      ],
      "metadata": {
        "id": "0140b4ce-feba-43ec-9a97-8c0ef53bf799"
      }
    },
    {
      "id": "b6b29253",
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import requests\n",
        "\n",
        "from transformers import AutoModel, AutoProcessor, get_cosine_schedule_with_warmup\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "data_path = '.'\n",
        "\n",
        "class DirectImageCaptionDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset that loads images and captions directly from files,\n",
        "       but only includes captioned images in the train split.\"\"\"\n",
        "\n",
        "    imagenette_classes = {\n",
        "        'n01440764': 'tench',\n",
        "        'n02102040': 'English springer',\n",
        "        'n02979186': 'cassette player',\n",
        "        'n03000684': 'chain saw',\n",
        "        'n03028079': 'church',\n",
        "        'n03394916': 'French horn',\n",
        "        'n03417042': 'garbage truck',\n",
        "        'n03425413': 'gas pump',\n",
        "        'n03445777': 'golf ball',\n",
        "        'n03888257': 'parachute',\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 imagenette_path: str,\n",
        "                 captions_path: str,\n",
        "                 split: str = 'train',\n",
        "                 transform=None,\n",
        "                 use_template: bool = False,\n",
        "                 n_classes: int = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            imagenette_path: Path to imagenette2 directory\n",
        "            captions_path: Path to imagenet_captions.json\n",
        "            split: 'train' or 'val'\n",
        "            transform: Optional transform to be applied to images\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.imagenette_path = Path(imagenette_path)\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        assert 1 <= n_classes <= 10, f\"Imagenette contains 10 classes. You cannot use {n_classes} classes.\"\n",
        "        self.imagenette_classes = {k: self.imagenette_classes[k] for k in list(self.imagenette_classes.keys())[:n_classes]}\n",
        "\n",
        "        # Load captions once\n",
        "        print(f\"Loading captions from {captions_path}...\")\n",
        "        with open(captions_path, 'r') as f:\n",
        "            captions_data = json.load(f)\n",
        "        # Map filename -> caption dict\n",
        "        self.filename_to_caption = {item['filename']: item for item in captions_data}\n",
        "\n",
        "        # Build image list, filtering out un-captioned images if train\n",
        "        self.image_paths: List[Path] = []\n",
        "        self._build_image_list()\n",
        "        self.use_template = use_template\n",
        "        print(f\"Found {len(self.image_paths)} images in '{self.split}' split\")\n",
        "\n",
        "    def _build_image_list(self):\n",
        "        split_dir = self.imagenette_path / self.split\n",
        "        for class_dir in split_dir.iterdir():\n",
        "            if not class_dir.is_dir():\n",
        "                continue\n",
        "            wnid = class_dir.name\n",
        "            if wnid not in self.imagenette_classes:\n",
        "                continue\n",
        "\n",
        "            for img_path in class_dir.iterdir():\n",
        "                if img_path.suffix.lower() not in ('.jpg', '.jpeg', '.png'):\n",
        "                    continue\n",
        "\n",
        "                if self.split == 'train':\n",
        "                    # only keep if we have a caption and wnid matches\n",
        "                    cap = self.filename_to_caption.get(img_path.name)\n",
        "                    if cap is not None and cap.get('wnid') == wnid:\n",
        "                        self.image_paths.append(img_path)\n",
        "                    # else: skip this image entirely\n",
        "                else:\n",
        "                    # val: keep all images, will fall back to template if no caption\n",
        "                    self.image_paths.append(img_path)\n",
        "\n",
        "    def _get_caption(self, img_path: Path) -> str:\n",
        "        \"\"\"Get or build a suitable caption for an image.\"\"\"\n",
        "        fn = img_path.name\n",
        "        wnid = img_path.parent.name\n",
        "\n",
        "        cap = self.filename_to_caption.get(fn)\n",
        "        if not self.use_template and cap is not None and cap.get('wnid') == wnid:\n",
        "            # build something like \"Title. tag1, tag2. description\"\n",
        "            parts = []\n",
        "            if cap.get('title'):\n",
        "                parts.append(cap['title'])\n",
        "            if cap.get('tags'):\n",
        "                tags = cap['tags'][:5]\n",
        "                parts.append(', '.join(tags))\n",
        "            if cap.get('description') and len(cap['description']) < 200:\n",
        "                parts.append(cap['description'])\n",
        "            caption = '. '.join(parts).replace('\\n',' ').strip()\n",
        "            return caption or f\"A photo of {self.imagenette_classes.get(wnid, wnid)}\"\n",
        "        else:\n",
        "            # fallback template\n",
        "            clsname = self.imagenette_classes.get(wnid, wnid)\n",
        "            return f\"A photo of {clsname}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        caption = self._get_caption(img_path)\n",
        "        return {\n",
        "            'image': image,\n",
        "            'caption': caption,\n",
        "            'image_path': str(img_path),\n",
        "        }\n",
        "\n",
        "def create_direct_data_loaders(imagenette_path: str,\n",
        "                               captions_path: str,\n",
        "                               batch_size: int = 32,\n",
        "                               use_template: bool = False,\n",
        "                               n_train_classes: int = 10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    train_ds = DirectImageCaptionDataset(\n",
        "        imagenette_path, captions_path, split='train', use_template=use_template,\n",
        "        transform=None, n_classes=n_train_classes\n",
        "    )\n",
        "    val_ds   = DirectImageCaptionDataset(\n",
        "        imagenette_path, captions_path, split='val', use_template=True,\n",
        "        transform=None, n_classes=n_train_classes\n",
        "    )\n",
        "\n",
        "    def collate_pil(batch):\n",
        "        \"\"\"Return a dict whose 'image' field is a list of PIL images (no stacking).\"\"\"\n",
        "        return {\n",
        "            \"image\": [item[\"image\"] for item in batch],\n",
        "            \"caption\": [item[\"caption\"] for item in batch],\n",
        "            \"image_path\": [item[\"image_path\"] for item in batch],\n",
        "        }\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=4, pin_memory=True, collate_fn=collate_pil, drop_last=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=4, pin_memory=True, collate_fn=collate_pil, drop_last=True)\n",
        "    return train_loader, val_loader, train_ds, val_ds\n",
        "\n",
        "\n",
        "\n",
        "_IMAGENET_TXT = (\n",
        "    \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        ")\n",
        "_IMAGENET_JSON = (\n",
        "    \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        ")\n",
        "\n",
        "assert torch.cuda.device_count() == 2, \"You need two GPUs for this exercise. You can use Kaggle, 30 hours per week are provided on 2x T4s\"\n",
        "\n",
        "def download_imagenet_label_lists() -> tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"Returns (`class_names`, `wnid2idx`).\"\"\"\n",
        "\n",
        "    # Human‑readable label strings (idx→name)\n",
        "    names = requests.get(_IMAGENET_TXT, timeout=30).text.strip().split(\"\\n\")\n",
        "\n",
        "    # Mapping WNID → integer class index\n",
        "    wnid2idx: Dict[str, int] = {}\n",
        "    j = requests.get(_IMAGENET_JSON, timeout=30).json()\n",
        "    for idx_str, (wnid, _) in j.items():\n",
        "        wnid2idx[wnid] = int(idx_str)\n",
        "\n",
        "    return names, wnid2idx\n",
        "\n",
        "\n",
        "imagenette_path = os.path.join(data_path, \"imagenette2\")\n",
        "captions_path  = os.path.join(data_path, \"imagenet_captions.json\")\n",
        "\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=256, use_template=False\n",
        ")\n",
        "\n",
        "print(f\"Train images (with captions only): {len(train_ds)}\")\n",
        "print(f\"Val images (all):               {len(val_ds)}\\n\")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Sample batch:\")\n",
        "# print(batch['image'].shape)\n",
        "for i, c in enumerate(batch['caption'][:3]):\n",
        "    print(f\"  {i}: {c}\")\n",
        "\n",
        "def build_text_embeddings(\n",
        "    processor, model, class_names: List[str], device: torch.device, batch_size: int = 128\n",
        "):\n",
        "    \"\"\"Encode *all* prompt texts once and stack into a single tensor.\"\"\"\n",
        "\n",
        "    prompts = [f\"This is a photo of a {name}.\" for name in class_names]\n",
        "    embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            batch_prompts = prompts[i : i + batch_size]\n",
        "            text_inputs = processor(  # padding *must* be max_length for SigLIP\n",
        "                # text=batch_prompts, padding=True, return_tensors=\"pt\", truncation=True\n",
        "                text=batch_prompts, padding=\"max_length\", return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "            emb = model.get_text_features(**text_inputs)\n",
        "            # emb = F.normalize(emb, dim=-1)\n",
        "            embeddings.append(emb)\n",
        "    return torch.cat(embeddings, dim=0)  # (1000, D)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:38:06.557205Z",
          "iopub.execute_input": "2025-07-03T12:38:06.557497Z",
          "iopub.status.idle": "2025-07-03T12:38:55.070490Z",
          "shell.execute_reply.started": "2025-07-03T12:38:06.557470Z",
          "shell.execute_reply": "2025-07-03T12:38:55.069476Z"
        },
        "id": "b6b29253",
        "outputId": "65e0ae0d-9b36-4f9b-9c9c-a8439800f3a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-07-03 12:38:26.902815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751546307.125598      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751546307.192614      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Loading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\nTrain images (with captions only): 3124\nVal images (all):               3925\n\nSample batch:\n  0: Golf ball. I have no golf club and I'm not a golfer but I have a golf ball\n  1: Luke in the hole. English Springer Spaniel, dog, Luke, puppy\n  2: Fisher-Price cassette player. fisher, price. see more: <a href=\"http://www.beatsnothing.com\" rel=\"noreferrer nofollow\">www.beatsnothing.com</a>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "b030dfde",
      "cell_type": "markdown",
      "source": [
        "### 0.b Training and evaluating SigLiP"
      ],
      "metadata": {
        "id": "b030dfde"
      }
    },
    {
      "id": "e01a71fd",
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    model: Optional[torch.nn.Module] = None,\n",
        "    model_name: str = \"google/siglip-base-patch16-224\",\n",
        "    epochs: int = 5,\n",
        "    lr: float = 5e-6,\n",
        "    warmup_steps: Optional[int] = None,\n",
        "    device: str | torch.device = \"cuda\",\n",
        "    eval_only: bool = False,\n",
        "    use_multi_gpu: bool = True,\n",
        "):\n",
        "    device = torch.device(device)\n",
        "\n",
        "    if warmup_steps is None:\n",
        "        warmup_steps = 0.1*len(train_loader)*epochs\n",
        "    print(\"\\nLoading model & processor …\")\n",
        "    model = model or AutoModel.from_pretrained(model_name).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    class_names, wnid2idx = download_imagenet_label_lists()\n",
        "    text_embeds = build_text_embeddings(processor, model, class_names, device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "    )\n",
        "    if use_multi_gpu:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(epochs + 1):\n",
        "\n",
        "        # -------- eval -------------------------------------------------------\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_total = 0\n",
        "        base_model = model.module if use_multi_gpu else model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "                wnids = [Path(p).parent.name for p in batch[\"image_path\"]]\n",
        "                true_indices = torch.tensor([wnid2idx[w] for w in wnids], device=device)\n",
        "\n",
        "                image_inputs = processor(images=images, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "                image_embeds = base_model.get_image_features(**image_inputs)\n",
        "\n",
        "                logits = image_embeds @ text_embeds.T  # (B, 1000)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                num_correct += (preds == true_indices).sum().item()\n",
        "                num_total += len(images)\n",
        "\n",
        "        acc = num_correct / num_total\n",
        "        print(f\"Epoch {epoch}/{epochs} – top‑1 Imagenette (1000‑way): {acc * 100:.2f}%\")\n",
        "        if eval_only or epoch >= epochs:\n",
        "            return base_model\n",
        "        # -------- training -------------------------------------------------------\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "            captions: List[str] = batch[\"caption\"]\n",
        "\n",
        "            inputs = processor(\n",
        "                text=captions,\n",
        "                images=images,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "            outputs = model(**inputs, return_loss=True)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if use_multi_gpu:\n",
        "                loss = loss.mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "        mean_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}/{epochs} – train loss: {mean_loss:.4f}\")\n",
        "\n",
        "    return base_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:38:55.071664Z",
          "iopub.execute_input": "2025-07-03T12:38:55.072251Z",
          "iopub.status.idle": "2025-07-03T12:38:55.084927Z",
          "shell.execute_reply.started": "2025-07-03T12:38:55.072220Z",
          "shell.execute_reply": "2025-07-03T12:38:55.084046Z"
        },
        "id": "e01a71fd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4c2c49d6",
      "cell_type": "markdown",
      "source": [
        "## 1. Fix the training evaluation"
      ],
      "metadata": {
        "id": "4c2c49d6"
      }
    },
    {
      "id": "0e965180-68a1-4593-9e8d-fdbf9a088804",
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    model: Optional[torch.nn.Module] = None,\n",
        "    model_name: str = \"google/siglip-base-patch16-224\",\n",
        "    epochs: int = 5,\n",
        "    lr: float = 5e-6,\n",
        "    warmup_steps: Optional[int] = None,\n",
        "    device: str | torch.device = \"cuda\",\n",
        "    eval_only: bool = False,\n",
        "    use_multi_gpu: bool = True,\n",
        "):\n",
        "    device = torch.device(device)\n",
        "\n",
        "    if warmup_steps is None:\n",
        "        warmup_steps = 0.1*len(train_loader)*epochs\n",
        "    print(\"\\nLoading model & processor …\")\n",
        "    model = model or AutoModel.from_pretrained(model_name).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    class_names, wnid2idx = download_imagenet_label_lists()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "    )\n",
        "    if use_multi_gpu:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(epochs + 1):\n",
        "\n",
        "        # -------- eval -------------------------------------------------------\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_total = 0\n",
        "        base_model = model.module if use_multi_gpu else model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_embeds = build_text_embeddings(processor, base_model, class_names, device) # Moved down in loop, as text embeds should update?\n",
        "            for batch in val_loader:\n",
        "                images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "                wnids = [Path(p).parent.name for p in batch[\"image_path\"]]\n",
        "                true_indices = torch.tensor([wnid2idx[w] for w in wnids], device=device)\n",
        "\n",
        "                image_inputs = processor(images=images, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "                image_embeds = base_model.get_image_features(**image_inputs)\n",
        "\n",
        "                logits = image_embeds @ text_embeds.T  # (B, 1000)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                num_correct += (preds == true_indices).sum().item()\n",
        "                num_total += len(images)\n",
        "\n",
        "        acc = num_correct / num_total\n",
        "        print(f\"Epoch {epoch}/{epochs} – top‑1 Imagenette (1000‑way): {acc * 100:.2f}%\")\n",
        "        if eval_only or epoch >= epochs:\n",
        "            return base_model\n",
        "        # -------- training -------------------------------------------------------\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "            captions: List[str] = batch[\"caption\"]\n",
        "\n",
        "            inputs = processor(\n",
        "                text=captions,\n",
        "                images=images,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "            outputs = model(**inputs, return_loss=True)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if use_multi_gpu:\n",
        "                loss = loss.mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "        mean_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}/{epochs} – train loss: {mean_loss:.4f}\")\n",
        "\n",
        "    return base_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T12:47:17.622496Z",
          "iopub.execute_input": "2025-07-03T12:47:17.622881Z",
          "iopub.status.idle": "2025-07-03T12:47:17.636985Z",
          "shell.execute_reply.started": "2025-07-03T12:47:17.622854Z",
          "shell.execute_reply": "2025-07-03T12:47:17.636277Z"
        },
        "id": "0e965180-68a1-4593-9e8d-fdbf9a088804"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2e72427c-fb91-4d5a-990a-b973f92a3533",
      "cell_type": "code",
      "source": [
        "from difflib import unified_diff\n",
        "\n",
        "# pick the execution numbers you want to compare\n",
        "left  = In[3].splitlines()   # cell with prompt In [3]:\n",
        "right = In[4].splitlines()   # cell with prompt In [5]:\n",
        "\n",
        "diff_lines = unified_diff(\n",
        "    left,\n",
        "    right,\n",
        "    fromfile='cell 3',\n",
        "    tofile='cell 4',\n",
        "    lineterm=''            # avoid extra blank lines\n",
        ")\n",
        "\n",
        "print('\\n'.join(diff_lines))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-03T12:42:26.791378Z",
          "iopub.execute_input": "2025-07-03T12:42:26.791755Z",
          "iopub.status.idle": "2025-07-03T12:42:26.798242Z",
          "shell.execute_reply.started": "2025-07-03T12:42:26.791716Z",
          "shell.execute_reply": "2025-07-03T12:42:26.797397Z"
        },
        "id": "2e72427c-fb91-4d5a-990a-b973f92a3533",
        "trusted": true,
        "outputId": "773b3bd5-3663-4daf-9591-a85dd028f784"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--- cell 3\n+++ cell 4\n@@ -19,7 +19,6 @@\n     processor = AutoProcessor.from_pretrained(model_name)\n \n     class_names, wnid2idx = download_imagenet_label_lists()\n-    text_embeds = build_text_embeddings(processor, model, class_names, device)\n \n     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0)\n     total_steps = epochs * len(train_loader)\n@@ -39,6 +38,7 @@\n         base_model = model.module if use_multi_gpu else model\n \n         with torch.no_grad():\n+            text_embeds = build_text_embeddings(processor, base_model, class_names, device) # Moved down in loop, as text embeds should update?\n             for batch in val_loader:\n                 images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n                 wnids = [Path(p).parent.name for p in batch[\"image_path\"]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "d82caa89",
      "cell_type": "markdown",
      "source": [
        "## 2. Finetuning SigLip"
      ],
      "metadata": {
        "id": "d82caa89"
      }
    },
    {
      "id": "23198065-5576-4152-a435-3d0f8b82b997",
      "cell_type": "markdown",
      "source": [
        "### 2.a) Using templates vs captions"
      ],
      "metadata": {
        "id": "23198065-5576-4152-a435-3d0f8b82b997"
      }
    },
    {
      "id": "74324ad6-353c-41ea-b61d-b61a489169f6",
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=False, n_train_classes=10\n",
        ")\n",
        "\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"\\nFinal validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader,)\n",
        "print(f\"\\nFinal validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T13:09:50.953456Z",
          "iopub.execute_input": "2025-07-03T13:09:50.954270Z",
          "iopub.status.idle": "2025-07-03T13:46:14.745187Z",
          "shell.execute_reply.started": "2025-07-03T13:09:50.954243Z",
          "shell.execute_reply": "2025-07-03T13:46:14.744228Z"
        },
        "id": "74324ad6-353c-41ea-b61d-b61a489169f6",
        "outputId": "74b71368-d707-4c6f-a223-40e08c1a4ec1",
        "colab": {
          "referenced_widgets": [
            "5d06df57f15e48ada085d77d4597cb9f",
            "eac40f0ad0fa4176985bfdfb9d32901d",
            "99ea56f3166d4ed59d075f997e27db1a",
            "705b27ac1f3d46e1b2d64b6e565c5f2a",
            "37b4c23a49cd4a609e0ade69dac7b1f3",
            "80649077025146e2a296072908d61bec",
            "99f12d62b3ba4b82ae7f20f1dfb04f2a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d06df57f15e48ada085d77d4597cb9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/813M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eac40f0ad0fa4176985bfdfb9d32901d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99ea56f3166d4ed59d075f997e27db1a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "705b27ac1f3d46e1b2d64b6e565c5f2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37b4c23a49cd4a609e0ade69dac7b1f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80649077025146e2a296072908d61bec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99f12d62b3ba4b82ae7f20f1dfb04f2a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 0/5 – top‑1 Imagenette (1000‑way): 77.79%\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 0/5 – train loss: 2.0811\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 70.13%\nEpoch 1/5 – train loss: 1.1454\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 72.64%\nEpoch 2/5 – train loss: 0.8410\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 70.90%\nEpoch 3/5 – train loss: 0.6911\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 69.88%\nEpoch 4/5 – train loss: 0.6261\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 69.49%\n\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 69.49%\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.79%\nEpoch 0/5 – train loss: 3.3809\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 84.02%\nEpoch 1/5 – train loss: 2.4460\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 86.73%\nEpoch 2/5 – train loss: 2.3920\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 86.91%\nEpoch 3/5 – train loss: 2.3677\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 86.48%\nEpoch 4/5 – train loss: 2.3774\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 86.40%\n\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 86.40%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "aacf6b76-7c31-4a96-b318-891ccd0a14dd",
      "cell_type": "markdown",
      "source": [
        "### 2.b) Explore the effect of batch size and number of classes"
      ],
      "metadata": {
        "id": "aacf6b76-7c31-4a96-b318-891ccd0a14dd"
      }
    },
    {
      "id": "00343696-c37d-4453-9eef-a25e6d9f641e",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "batch_size = 128\n",
        "print(f\"{'+'*100} \\nUsing batch size of: {batch_size} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T13:50:42.022315Z",
          "iopub.execute_input": "2025-07-03T13:50:42.022630Z",
          "iopub.status.idle": "2025-07-03T14:08:34.817755Z",
          "shell.execute_reply.started": "2025-07-03T13:50:42.022614Z",
          "shell.execute_reply": "2025-07-03T14:08:34.816740Z"
        },
        "id": "00343696-c37d-4453-9eef-a25e6d9f641e",
        "outputId": "3266d4cd-5c25-42bb-bde2-c5b59c822f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch size of: 128 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 78.05%\nEpoch 0/5 – train loss: 5.2859\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 82.76%\nEpoch 1/5 – train loss: 3.2692\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 86.22%\nEpoch 2/5 – train loss: 3.0853\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 86.64%\nEpoch 3/5 – train loss: 3.0743\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 86.95%\nEpoch 4/5 – train loss: 3.0524\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 86.95%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 86.95%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "c925e1f3-657a-43fe-8011-fd443de5d559",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch size of: {batch_size} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "c925e1f3-657a-43fe-8011-fd443de5d559"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "466d88f5-6c72-4110-bfc5-944ce73fcaba",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "batch_size = 16\n",
        "print(f\"{'+'*100} \\nUsing batch size of: {batch_size} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T14:18:42.224624Z",
          "iopub.execute_input": "2025-07-03T14:18:42.226113Z",
          "iopub.status.idle": "2025-07-03T14:39:25.024618Z",
          "shell.execute_reply.started": "2025-07-03T14:18:42.226079Z",
          "shell.execute_reply": "2025-07-03T14:39:25.023535Z"
        },
        "id": "466d88f5-6c72-4110-bfc5-944ce73fcaba",
        "outputId": "6725b384-2ed6-44e4-fa4b-b04b458225f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch size of: 16 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.73%\nEpoch 0/5 – train loss: 1.5715\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 87.58%\nEpoch 1/5 – train loss: 1.2983\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 89.16%\nEpoch 2/5 – train loss: 1.2574\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 90.46%\nEpoch 3/5 – train loss: 1.2617\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 91.25%\nEpoch 4/5 – train loss: 1.2339\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 91.05%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 91.05%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "9a3d292b-4ac5-4cbd-a2fc-352fa19f2817",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "batch_size = 8\n",
        "print(f\"{'+'*100} \\nUsing batch size of: {batch_size} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T14:41:13.580106Z",
          "iopub.execute_input": "2025-07-03T14:41:13.581616Z",
          "iopub.status.idle": "2025-07-03T15:05:13.053081Z",
          "shell.execute_reply.started": "2025-07-03T14:41:13.581580Z",
          "shell.execute_reply": "2025-07-03T15:05:13.052152Z"
        },
        "id": "9a3d292b-4ac5-4cbd-a2fc-352fa19f2817",
        "outputId": "8996036e-792d-453a-eb4f-0922f31da6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch size of: 8 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.73%\nEpoch 0/5 – train loss: 1.0309\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 89.72%\nEpoch 1/5 – train loss: 0.8393\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 93.09%\nEpoch 2/5 – train loss: 0.7668\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 90.28%\nEpoch 3/5 – train loss: 0.7438\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 93.42%\nEpoch 4/5 – train loss: 0.7157\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 93.44%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 93.44%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "055d8de6-6360-49b5-b251-9a3855670377",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "batch_size = 4\n",
        "print(f\"{'+'*100} \\nUsing batch size of: {batch_size} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=10\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:01:46.620478Z",
          "iopub.execute_input": "2025-07-03T17:01:46.620777Z",
          "iopub.status.idle": "2025-07-03T17:11:37.160946Z",
          "shell.execute_reply.started": "2025-07-03T17:01:46.620750Z",
          "shell.execute_reply": "2025-07-03T17:11:37.159804Z"
        },
        "id": "055d8de6-6360-49b5-b251-9a3855670377",
        "outputId": "b7dfb9e0-c8fd-49ee-fb34-c44cc5c0b1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch size of: 4 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.68%\nEpoch 0/5 – train loss: 0.5712\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 93.55%\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35/1466052484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimagenette_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final validation accuracy using templates:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_35/3855554455.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, model_name, epochs, lr, warmup_steps, device, eval_only, use_multi_gpu)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "id": "7765f75a-1b09-43d9-82a9-1f18b9ff0f16",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "n_train_classes = 1\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:12:06.520191Z",
          "iopub.execute_input": "2025-07-03T17:12:06.520757Z",
          "iopub.status.idle": "2025-07-03T17:14:29.159743Z",
          "shell.execute_reply.started": "2025-07-03T17:12:06.520734Z",
          "shell.execute_reply": "2025-07-03T17:14:29.158852Z"
        },
        "id": "7765f75a-1b09-43d9-82a9-1f18b9ff0f16",
        "outputId": "6802b272-c4ea-4eb7-ffa1-205eeea7b435"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 1 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 154 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 387 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 89.06%\nEpoch 0/5 – train loss: 89.6434\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 28.12%\nEpoch 1/5 – train loss: 10.4061\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 0.78%\nEpoch 2/5 – train loss: 14.2488\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 0.52%\nEpoch 3/5 – train loss: 13.7850\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 1.04%\nEpoch 4/5 – train loss: 12.9667\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 1.30%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 1.30%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "35026adc-cd5b-4ea5-bc5d-d8a2183b2eec",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "n_train_classes = 2\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:11:37.162993Z",
          "iopub.status.idle": "2025-07-03T17:11:37.163346Z",
          "shell.execute_reply.started": "2025-07-03T17:11:37.163172Z",
          "shell.execute_reply": "2025-07-03T17:11:37.163189Z"
        },
        "id": "35026adc-cd5b-4ea5-bc5d-d8a2183b2eec"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "322dd14f-7533-4e7c-9dad-7144f82f6499",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "n_train_classes = 4\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:11:37.164444Z",
          "iopub.status.idle": "2025-07-03T17:11:37.164740Z",
          "shell.execute_reply.started": "2025-07-03T17:11:37.164559Z",
          "shell.execute_reply": "2025-07-03T17:11:37.164575Z"
        },
        "id": "322dd14f-7533-4e7c-9dad-7144f82f6499"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8423fe1e-2e4d-459f-8d66-b6ef32c1d76f",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "n_train_classes = 8\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=64, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:11:37.165877Z",
          "iopub.status.idle": "2025-07-03T17:11:37.166092Z",
          "shell.execute_reply.started": "2025-07-03T17:11:37.165985Z",
          "shell.execute_reply": "2025-07-03T17:11:37.165994Z"
        },
        "id": "8423fe1e-2e4d-459f-8d66-b6ef32c1d76f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6a49d38-e292-4fce-97e1-cc1dbd69a999",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "n_train_classes = 4\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = train(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = train(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T15:49:40.821428Z",
          "iopub.execute_input": "2025-07-03T15:49:40.822003Z",
          "iopub.status.idle": "2025-07-03T15:57:03.031994Z",
          "shell.execute_reply.started": "2025-07-03T15:49:40.821975Z",
          "shell.execute_reply": "2025-07-03T15:57:03.030742Z"
        },
        "id": "c6a49d38-e292-4fce-97e1-cc1dbd69a999",
        "outputId": "3d021692-2c0d-48b2-b8aa-a5e933949cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 4 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 1069 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 1525 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 71.48%\nEpoch 0/5 – train loss: 4.0838\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 83.44%\nEpoch 1/5 – train loss: 2.6809\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 86.97%\nEpoch 2/5 – train loss: 2.6245\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 86.97%\nEpoch 3/5 – train loss: 2.5890\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 87.23%\nEpoch 4/5 – train loss: 2.5678\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 87.23%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 87.23%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "e4492e21",
      "cell_type": "markdown",
      "source": [
        "## 3. Finetuning SigLiT"
      ],
      "metadata": {
        "id": "e4492e21"
      }
    },
    {
      "id": "c4424893",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "def fine_tune_siglit_style(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    model: Optional[torch.nn.Module] = None,\n",
        "    model_name: str = \"google/siglip-base-patch16-224\",\n",
        "    epochs: int = 5,\n",
        "    lr: float = 5e-6,\n",
        "    warmup_steps: Optional[int] = None,\n",
        "    device: str | torch.device = \"cuda\",\n",
        "    eval_only: bool = False,\n",
        "    use_multi_gpu: bool = True,\n",
        "):\n",
        "    device = torch.device(device)\n",
        "\n",
        "    if warmup_steps is None:\n",
        "        warmup_steps = 0.1*len(train_loader)*epochs\n",
        "    print(\"\\nLoading model & processor …\")\n",
        "    model = model or AutoModel.from_pretrained(model_name).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    # ❄️❄️❄️freeze the vision tower❄️❄️❄️\n",
        "    for parameter in model.vision_model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "\n",
        "\n",
        "    # only optimize text tower\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "    class_names, wnid2idx = download_imagenet_label_lists()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    if use_multi_gpu:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(epochs + 1):\n",
        "\n",
        "        base_model = model.module if use_multi_gpu else model\n",
        "\n",
        "        # -------- eval -------------------------------------------------------\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        num_total = 0\n",
        "        base_model = model.module if use_multi_gpu else model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_embeds = build_text_embeddings(processor, base_model, class_names, device)\n",
        "\n",
        "            for batch in val_loader:\n",
        "                images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "                wnids = [Path(p).parent.name for p in batch[\"image_path\"]]\n",
        "                true_indices = torch.tensor([wnid2idx[w] for w in wnids], device=device)\n",
        "\n",
        "                image_inputs = processor(images=images, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
        "                image_embeds = base_model.get_image_features(**image_inputs)\n",
        "\n",
        "                logits = image_embeds @ text_embeds.T  # (B, 1000)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                num_correct += (preds == true_indices).sum().item()\n",
        "                num_total += len(images)\n",
        "\n",
        "        acc = num_correct / num_total\n",
        "        print(f\"Epoch {epoch}/{epochs} – top‑1 Imagenette (1000‑way): {acc * 100:.2f}%\")\n",
        "        if eval_only or epoch >= epochs:\n",
        "            return base_model\n",
        "        # -------- training -------------------------------------------------------\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images: List[\"PIL.Image.Image\"] = batch[\"image\"]\n",
        "            captions: List[str] = batch[\"caption\"]\n",
        "\n",
        "            inputs = processor(\n",
        "                text=captions,\n",
        "                images=images,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "            outputs = model(**inputs, return_loss=True)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if use_multi_gpu:\n",
        "                loss = loss.mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "        mean_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}/{epochs} – train loss: {mean_loss:.4f}\")\n",
        "\n",
        "    return base_model\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:55:38.209098Z",
          "iopub.execute_input": "2025-07-03T17:55:38.209376Z",
          "iopub.status.idle": "2025-07-03T17:55:38.223436Z",
          "shell.execute_reply.started": "2025-07-03T17:55:38.209359Z",
          "shell.execute_reply": "2025-07-03T17:55:38.222673Z"
        },
        "id": "c4424893"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6becaab3-7f3a-4371-b8bc-72b8826793b2",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 128\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:41:02.237525Z",
          "iopub.execute_input": "2025-07-03T18:41:02.237967Z",
          "iopub.status.idle": "2025-07-03T18:56:11.686349Z",
          "shell.execute_reply.started": "2025-07-03T18:41:02.237933Z",
          "shell.execute_reply": "2025-07-03T18:56:11.685448Z"
        },
        "id": "6becaab3-7f3a-4371-b8bc-72b8826793b2",
        "outputId": "678f8542-0f39-4304-b1da-0897bb9338ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch_size of: 128 and template\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 10 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 78.05%\nEpoch 0/5 – train loss: 5.3876\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 76.64%\nEpoch 1/5 – train loss: 3.5065\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 80.83%\nEpoch 2/5 – train loss: 3.2617\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 81.51%\nEpoch 3/5 – train loss: 3.2195\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 81.43%\nEpoch 4/5 – train loss: 3.1813\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 81.54%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 81.54%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "76370fe8-b7a3-4e53-be48-2be7fec30302",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 64\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:16:25.254417Z",
          "iopub.execute_input": "2025-07-03T18:16:25.255602Z",
          "iopub.status.idle": "2025-07-03T18:31:45.752123Z",
          "shell.execute_reply.started": "2025-07-03T18:16:25.255567Z",
          "shell.execute_reply": "2025-07-03T18:31:45.751243Z"
        },
        "id": "76370fe8-b7a3-4e53-be48-2be7fec30302",
        "outputId": "1b067c5f-a27a-401b-ec65-3dc47a3e5ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch_size of: 64 and template\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 10 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.79%\nEpoch 0/5 – train loss: 3.4323\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 79.59%\nEpoch 1/5 – train loss: 2.5941\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 80.20%\nEpoch 2/5 – train loss: 2.5091\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 80.71%\nEpoch 3/5 – train loss: 2.4905\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 80.66%\nEpoch 4/5 – train loss: 2.4710\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 80.74%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 80.74%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "486efa40-ee58-4e20-a3cf-afb0e1c626a9",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T17:55:40.408110Z",
          "iopub.execute_input": "2025-07-03T17:55:40.408895Z",
          "iopub.status.idle": "2025-07-03T18:11:33.554891Z",
          "shell.execute_reply.started": "2025-07-03T17:55:40.408868Z",
          "shell.execute_reply": "2025-07-03T18:11:33.553994Z"
        },
        "id": "486efa40-ee58-4e20-a3cf-afb0e1c626a9",
        "outputId": "c1839a3d-84cd-4b2e-f6ce-1fa1fa5ad766"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 10 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.79%\nEpoch 0/5 – train loss: 2.3395\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 83.66%\nEpoch 1/5 – train loss: 1.9254\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 83.20%\nEpoch 2/5 – train loss: 1.8958\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 82.33%\nEpoch 3/5 – train loss: 1.8720\nEpoch 4/5 – top‑1 Imagenette (1000‑way): 82.48%\nEpoch 4/5 – train loss: 1.8036\nEpoch 5/5 – top‑1 Imagenette (1000‑way): 83.02%\nFinal validation accuracy using templates:\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 83.02%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "e2bcb917-66c6-4274-bb13-1fcc90f6b610",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 16\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-03T18:57:15.294429Z",
          "iopub.execute_input": "2025-07-03T18:57:15.294847Z",
          "execution_failed": "2025-07-03T20:16:51.388Z"
        },
        "id": "e2bcb917-66c6-4274-bb13-1fcc90f6b610",
        "outputId": "375ecb79-2132-4007-a8c4-381da6b2f6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing batch_size of: 16 and template\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \nUsing n_train_classes of: 10 and template\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n Using template \n %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLoading captions from ./imagenet_captions.json...\nFound 3124 images in 'train' split\nLoading captions from ./imagenet_captions.json...\nFound 3925 images in 'val' split\n\nLoading model & processor …\nEpoch 0/5 – top‑1 Imagenette (1000‑way): 77.73%\nEpoch 0/5 – train loss: 1.6496\nEpoch 1/5 – top‑1 Imagenette (1000‑way): 82.65%\nEpoch 1/5 – train loss: 1.4073\nEpoch 2/5 – top‑1 Imagenette (1000‑way): 81.84%\nEpoch 2/5 – train loss: 1.3022\nEpoch 3/5 – top‑1 Imagenette (1000‑way): 80.61%\nEpoch 3/5 – train loss: 1.2668\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "5e4286c8-be9e-49c2-b36b-e4778c4abb65",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 8\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "5e4286c8-be9e-49c2-b36b-e4778c4abb65"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e6523af-2ffd-4f08-add7-04a73d81d1b8",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 10\n",
        "batch_size = 4\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "5e6523af-2ffd-4f08-add7-04a73d81d1b8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3b4bb22b-38c7-4c0b-8a8c-eefe3ba48da4",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 8\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "3b4bb22b-38c7-4c0b-8a8c-eefe3ba48da4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31b30315-ec12-4177-8b68-1ecc9c6f1833",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 6\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "31b30315-ec12-4177-8b68-1ecc9c6f1833"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ce1601e4-9463-45a7-823a-e4cb3b608d1c",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 4\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "ce1601e4-9463-45a7-823a-e4cb3b608d1c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a40057f5-4e29-4ca5-a4dc-78da5ea31333",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 2\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "a40057f5-4e29-4ca5-a4dc-78da5ea31333"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2259ae9-992c-4021-b603-048a7b59b045",
      "cell_type": "code",
      "source": [
        "# ---- YOUR CODE STARTS HERE ----\n",
        "# Fine tuning SigLit model\n",
        "n_train_classes = 1\n",
        "batch_size = 32\n",
        "print(f\"{'+'*100} \\nUsing batch_size of: {batch_size} and template\")\n",
        "print(f\"{'+'*100} \\nUsing n_train_classes of: {n_train_classes} and template\")\n",
        "print(f\"{'%'*100}\\n Using template \\n {'%'*100}\")\n",
        "train_loader, val_loader, train_ds, val_ds = create_direct_data_loaders(\n",
        "    imagenette_path, captions_path, batch_size=batch_size, use_template=True, n_train_classes=n_train_classes\n",
        ")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, )\n",
        "print(f\"Final validation accuracy using templates:\")\n",
        "model = fine_tune_siglit_style(train_loader, val_loader, model=model, eval_only=True)\n",
        "# ---- YOUR CODE ENDS HERE ----"
      ],
      "metadata": {
        "trusted": true,
        "id": "a2259ae9-992c-4021-b603-048a7b59b045"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}